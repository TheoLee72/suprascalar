use anyhow::{Error as E, Result};
use candle_core::backend::BackendDevice;
use candle_core::{Device, IndexOp, Tensor};

// Assuming you have the patched Qwen3 or wrapper with forward_speculative
use candle_transformers::models::quantized_qwen2::ModelWeights as Qwen2;
use suprascalar::candle_transformers_patched::quantized_qwen3::ModelWeights as Qwen3;

use hf_hub::api::sync::Api;
use std::io::Write;
use std::time::{Duration, Instant};
use tokenizers::Tokenizer;

enum Model {
    Qwen2(Qwen2),
    Qwen3(Qwen3),
}

impl Model {
    fn forward(&mut self, x: &Tensor, offset: usize) -> Result<Tensor> {
        match self {
            Model::Qwen2(m) => m.forward(x, offset).map_err(E::from),
            Model::Qwen3(m) => m.forward(x, offset).map_err(E::from),
        }
    }
    // Assumes this returns [Batch, Seq_Len, Vocab]
    fn forward_speculative(&mut self, x: &Tensor, offset: usize) -> Result<Tensor> {
        match self {
            Model::Qwen2(m) => m.forward(x, offset).map_err(E::from),
            // Model::Qwen3(m) => m.forward_speculative(x, offset).map_err(E::from),
            Model::Qwen3(m) => m.forward(x, offset).map_err(E::from),
        }
    }
}

// ... [ModelType, Engine struct, Engine::new implementations are same as before] ...
enum ModelType {
    Qwen2,
    Qwen3,
}
struct Engine {
    model: Model,
    device: Device,
}
impl Engine {
    fn new(repo: &str, model_file: &str, device: &Device, model_type: ModelType) -> Result<Self> {
        let api = Api::new()?;
        let model_path = api.model(repo.to_string()).get(model_file)?;
        let mut file = std::fs::File::open(&model_path)?;
        let content = candle_core::quantized::gguf_file::Content::read(&mut file)?;
        let model = match model_type {
            ModelType::Qwen2 => Model::Qwen2(Qwen2::from_gguf(content, &mut file, device)?),
            ModelType::Qwen3 => Model::Qwen3(Qwen3::from_gguf(content, &mut file, device)?),
        };
        Ok(Self {
            model,
            device: device.clone(),
        })
    }
}

#[derive(Default)]
struct PerfStats {
    draft_forward: Duration,
    verifier_chunk: Duration,
    // ì „ì²´ Step 4(ë³´ë„ˆìŠ¤/ì‹±í¬ í¬í•¨) íƒ€ì´ë°
    verifier_resync: Duration,
    // verifier.model.forward(...) + verifier device sync ë§Œ ì¸¡ì •í•œ íƒ€ì´ë°
    verifier_resync_verifier_only: Duration,
}

fn sync_device(device: &Device) -> Result<()> {
    match device {
        Device::Cuda(dev) => dev.synchronize().map_err(E::from),
        _ => Ok(()),
    }
}

fn run_speculative(
    draft: &mut Engine,
    verifier: &mut Engine,
    tokenizer: &Tokenizer,
    prompt: &str,
    n_tokens: usize,
    k_draft: usize,
) -> Result<()> {
    println!("\nğŸš€ Speculative Decoding (GPU-Resident Optimization)");
    println!("Prompt: {}\n---", prompt);

    let mut tokens = tokenizer
        .encode(prompt, true)
        .map_err(E::msg)?
        .get_ids()
        .to_vec();
    let mut generated_cnt = 0;
    let mut draft_pos = 0;
    let mut verifier_pos = 0;
    let mut last_printed = 0;
    let mut total_drafted = 0;
    let mut total_draft_accepted = 0;
    let mut total_positions_accepted = 0;
    let mut total_bonus = 0;
    let mut stats = PerfStats::default();
    // Count verifier forwards (including speculative)
    let mut verifier_forward_count_total: usize = 0;
    let mut verifier_forward_speculative_count: usize = 0;

    let mut current_k = k_draft.max(1);
    const MAX_K: usize = 8;
    const MIN_K: usize = 1;
    const ADJUST_WINDOW: usize = 12;
    let mut adjust_acc_sum = 0f32;
    let mut adjust_cnt = 0usize;

    print!("{}", prompt);
    std::io::stdout().flush()?;

    // 1. Initial Prompt Processing (Prefill)
    // We treat this normally to get the KV cache ready
    let input = Tensor::new(tokens.as_slice(), &verifier.device)?.unsqueeze(0)?;

    // Draft Prefill
    let t_pre = Instant::now();
    let draft_prefill_logits = draft.model.forward(&input, 0)?;
    sync_device(&draft.device)?;
    stats.draft_forward += t_pre.elapsed();

    let mut last_draft_logits = draft_prefill_logits.squeeze(0)?;
    // [Optimized] Keep token on GPU to avoid Sync
    let mut draft_init_token_tensor = last_draft_logits.argmax(0)?.reshape((1, 1))?;

    // ğŸ”¥ ì¤‘ìš”: ì²« í„´ì˜ Verifier ê²°ê³¼(Logits)ë¥¼ ì €ì¥í•´ë‘¬ì•¼ í•¨ (ì²« Draft ê²€ì¦ìš©)
    // let t_pre_v = Instant::now();
    // Extract the last token from `input` as a [1, 1] tensor on the verifier device
    let mut bonus_token_tensor = input
        .narrow(1, tokens.len().saturating_sub(1), 1)?
        .reshape((1, 1))?;
    let _ = verifier.model.forward(&input, 0)?;
    verifier_forward_count_total += 1;
    // sync_device(&verifier.device)?;
    // stats.verifier_chunk += t_pre_v.elapsed();

    // let mut last_verifier_logits = logits.squeeze(0)?; // [vocab]

    draft_pos += tokens.len();
    verifier_pos += tokens.len() - 1;

    while generated_cnt < n_tokens {
        let remaining = n_tokens - generated_cnt;
        let step_k = remaining.min(current_k).max(1);

        // ================================================================
        // Step 1: Sequential Drafting (ğŸ”¥ GPU-Resident Loop Optimized)
        // ================================================================
        // CPU ëŒ€ê¸°(Sync) ì—†ì´ GPU ì•ˆì—ì„œë§Œ í…ì„œë¥¼ ëŒë¦½ë‹ˆë‹¤.
        let t_draft = Instant::now();

        // 1. ì´ˆê¸° í† í° ì„¤ì • (GPU Resident)
        // draft_init_token_tensor is already [1, 1] on GPU
        let mut current_input = draft_init_token_tensor.clone(); //êµ³ì´ í•„ìš”ì—†ì„ ìˆ˜ë„

        // [Optimized] Pre-allocate verify_input_gpu
        // We need to store [init, draft_1, draft_2, ...]
        // Total length = step_k
        // We can use a pre-allocated tensor and update it.
        // Note: DType must match. Tokenizer produces u32.
        // But draft_init_token_tensor is u32?
        // Let's check dtype.
        let dtype = current_input.dtype();
        let mut verify_input_gpu = Tensor::zeros((1, step_k + 1), dtype, &draft.device)?;

        // Set first token
        verify_input_gpu = verify_input_gpu.slice_assign(&[0..1, 0..1], &bonus_token_tensor)?;
        verify_input_gpu = verify_input_gpu.slice_assign(&[0..1, 1..2], &current_input)?;

        for i in 1..step_k {
            // A. Forward (Async Kernel Launch)
            let logits = draft.model.forward(&current_input, draft_pos)?;

            // B. Argmax (GPU Operation)
            let next_token_tensor = logits.squeeze(0)?.argmax(0)?.reshape((1, 1))?;

            // C. ì €ì¥ (In-place update)
            verify_input_gpu =
                verify_input_gpu.slice_assign(&[0..1, i + 1..i + 2], &next_token_tensor)?;

            // D. ë‹¤ìŒ ì…ë ¥ ì¤€ë¹„
            current_input = next_token_tensor;

            draft_pos += 1;
        }
        sync_device(&draft.device)?;
        stats.draft_forward += t_draft.elapsed();

        // ================================================================
        // Step 2: Verifier ë³‘ë ¬ ê²€ì¦
        // ================================================================
        let t_verify = Instant::now();

        // verify_input_gpu is already ready!

        // 2. í˜„ì¬ posì—ì„œ forward
        let verifier_logits = verifier
            .model
            .forward_speculative(&verify_input_gpu, verifier_pos)?;
        verifier_forward_count_total += 1;
        verifier_forward_speculative_count += 1;

        sync_device(&verifier.device)?;
        stats.verifier_chunk += t_verify.elapsed();

        let verifier_logits = verifier_logits.squeeze(0)?; // [step_k, vocab]

        // ================================================================
        // Step 3: Comparison Loop (Vectorized Logic)
        // ================================================================

        // [Optimized] ì´ì œ ì—¬ê¸°ì„œ í•œ ë²ˆì— CPUë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤ (Batch Sync)
        let draft_tokens = verify_input_gpu
            .squeeze(0)?
            .narrow(0, 1, step_k)?
            .to_vec1::<u32>()?;

        // ref_logits : [step_k, vocab]
        let ref_logits = verifier_logits.narrow(0, 0, step_k)?;

        // pred_tokens : [step_k]
        let pred_tokens = ref_logits.argmax(1)?;
        let pred_tokens = pred_tokens.to_vec1::<u32>()?;

        // ìµœì´ˆ ë¶ˆì¼ì¹˜ ì§€ì  ì°¾ê¸°
        let mismatch_idx = draft_tokens
            .iter()
            .zip(pred_tokens.iter())
            .position(|(draft_tok, pred_tok)| draft_tok != pred_tok);

        let mut accepted_from_draft = 0usize;
        let mut positions_advanced;
        let mut final_token: Option<u32> = None;

        match mismatch_idx {
            Some(idx) => {
                // ì•ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìˆ˜ë½
                if idx > 0 {
                    tokens.extend_from_slice(&draft_tokens[..idx]);
                    accepted_from_draft += idx;
                }
                // ë¶ˆì¼ì¹˜ ì§€ì ì—ì„œëŠ” Verifier í† í°ìœ¼ë¡œ êµì²´
                let replace_tok = pred_tokens[idx];
                tokens.push(replace_tok);
                final_token = Some(replace_tok);
            }
            None => {
                // ì „ë¶€ ì¼ì¹˜ â†’ ëª¨ë‘ ìˆ˜ë½
                tokens.extend_from_slice(&draft_tokens);
                accepted_from_draft += draft_tokens.len();
            }
        }
        positions_advanced = accepted_from_draft + usize::from(final_token.is_some());

        // ================================================================
        // Step 4: Bonus Token or Sync
        // - We measure two timings:
        //   1) `t_resync` : total time for the Step 4 block (legacy)
        //   2) `verifier-only` : only the verifier.model.forward + its device sync
        // ================================================================
        let t_resync = Instant::now();
        if final_token.is_none() {
            // All Accepted! -> Bonus Token
            let bonus_logits = verifier_logits.i(step_k)?;
            let bonus_token = bonus_logits.argmax(0)?.to_scalar::<u32>()?;
            bonus_token_tensor = bonus_logits.argmax(0)?.reshape((1, 1))?;

            tokens.push(bonus_token);
            positions_advanced += 1;
            total_bonus += 1;

            // ë‹¤ìŒ í„´ì„ ìœ„í•´ Logit ì €ì¥ (verifier-only timing ì¸¡ì •)
            // let input = Tensor::new(&[bonus_token], &verifier.device)?.unsqueeze(0)?;
            // let t_verifier_only = Instant::now();
            // let logits = verifier.model.forward(&input, verifier_pos + step_k)?; //ì—¬ê¸°~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            // sync_device(&verifier.device)?;
            // stats.verifier_resync_verifier_only += t_verifier_only.elapsed();
            // last_verifier_logits = logits.squeeze(0)?;

            // Draft ëª¨ë¸ ì‹±í¬ ë§ì¶”ê¸°
            let len = tokens.len();
            let last_two = &tokens[len - 2..len];
            let input = Tensor::new(last_two, &draft.device)?.unsqueeze(0)?;
            last_draft_logits = draft.model.forward(&input, draft_pos)?;
            sync_device(&draft.device)?;
            last_draft_logits = last_draft_logits.squeeze(0)?;
            // [Optimized] Keep as Tensor
            draft_init_token_tensor = last_draft_logits.argmax(0)?.reshape((1, 1))?;
            draft_pos += 2;
        } else {
            // Rejected -> Correction & Sync
            let accepted_idx = accepted_from_draft;

            // Draft ëª¨ë¸ ì‹±í¬ ë§ì¶”ê¸° & ë‹¤ìŒ í„´ ê²€ì¦ìš© Logit ê³„ì‚°
            let correct_token = final_token.unwrap();
            let input = Tensor::new(&[correct_token], &verifier.device)?.unsqueeze(0)?;

            // Sync Draft Model State (not included in verifier-only timing)
            let draft_input = input.clone();
            last_draft_logits = draft
                .model
                .forward(&draft_input, verifier_pos + accepted_idx)?;
            sync_device(&draft.device)?;
            last_draft_logits = last_draft_logits.squeeze(0)?;
            // [Optimized] Keep as Tensor
            draft_init_token_tensor = last_draft_logits.argmax(0)?.reshape((1, 1))?;

            // Reset Draft Pos to correct position
            draft_pos = verifier_pos + accepted_idx + 1;

            // Verifier: ë‹¤ìŒ í„´ ê²€ì¦ìš© Logit ê³„ì‚° (verifier-only timing)
            bonus_token_tensor = input;
            // let t_verifier_only = Instant::now();
            // let logits = verifier
            // .model
            // .forward(&input, verifier_pos + accepted_idx)?; //ì—¬ê¸°~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            // sync_device(&verifier.device)?;
            // stats.verifier_resync_verifier_only += t_verifier_only.elapsed();
            // last_verifier_logits = logits.squeeze(0)?;
        }
        // ê¸°ì¡´ì˜ total Step 4 íƒ€ì´ë°ë„ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
        stats.verifier_resync += t_resync.elapsed();

        total_drafted += step_k;
        total_draft_accepted += accepted_from_draft;
        total_positions_accepted += positions_advanced;

        verifier_pos += positions_advanced;
        generated_cnt += positions_advanced;

        // Print
        let text = tokenizer
            .decode(&tokens[last_printed..], true)
            .map_err(E::msg)?;
        if !text.is_empty() {
            print!("{}", text);
            std::io::stdout().flush()?;
            last_printed = tokens.len();
        }

        let acc_ratio = accepted_from_draft as f32 / step_k as f32;
        adjust_acc_sum += acc_ratio;
        adjust_cnt += 1;
        if adjust_cnt == ADJUST_WINDOW {
            let avg = adjust_acc_sum / ADJUST_WINDOW as f32;
            if avg > 0.6 && current_k < MAX_K {
                current_k += 1;
                println!(
                    "\nâ¬†ï¸ Increasing speculative window to {} (avg acceptance {:.0}%)",
                    current_k,
                    avg * 100.0
                );
            } else if avg < 0.4 && current_k > MIN_K {
                current_k -= 1;
                println!(
                    "\nâ¬‡ï¸ Decreasing speculative window to {} (avg acceptance {:.0}%)",
                    current_k,
                    avg * 100.0
                );
            }
            adjust_acc_sum = 0.0;
            adjust_cnt = 0;
        }
    }

    println!("\n\nDone.");
    let rate = (total_draft_accepted as f32 / total_drafted as f32) * 100.0;
    println!(
        "Acceptance Rate (draft only): {:.2}% | Bonus tokens: {} | Total advanced: {}",
        rate, total_bonus, total_positions_accepted
    );
    println!(
        "â±ï¸ draft {:.2?} | verifier-batch {:.2?} | verifier-sync-total {:.2?} | verifier-sync-verifier_only {:.2?}",
        stats.draft_forward,
        stats.verifier_chunk,
        stats.verifier_resync,
        stats.verifier_resync_verifier_only
    );
    println!(
        "Verifier forward calls: total={} (speculative={})",
        verifier_forward_count_total, verifier_forward_speculative_count
    );
    Ok(())
}

#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸ”¥ Speculative Decoding (Batch Verification + GPU Resident)");

    let device = Device::new_cuda(0)?;
    let api = Api::new()?;

    let tokenizer_path = api
        .model("Qwen/Qwen3-14B".to_string())
        .get("tokenizer.json")?;
    let tokenizer = Tokenizer::from_file(tokenizer_path).map_err(E::msg)?;

    let mut verifier = Engine::new(
        "unsloth/Qwen3-14B-GGUF",
        "Qwen3-14B-Q4_K_M.gguf",
        &device,
        ModelType::Qwen3,
    )?;

    let mut draft = Engine::new(
        "unsloth/Qwen3-0.6B-GGUF",
        "Qwen3-0.6B-Q4_K_M.gguf",
        &device,
        ModelType::Qwen3,
    )?;

    let prompt = "Explain the difference between Mutex and RwLock in Rust.";
    let start = std::time::Instant::now();

    // k_draftëŠ” ì´ˆê¸°ê°’ì¼ ë¿ì´ë©° ë£¨í”„ ë‚´ë¶€ì—ì„œ ìˆ˜ìš©ë¥ ì— ë”°ë¼ ìë™ ì¡°ì •ë©ë‹ˆë‹¤.
    run_speculative(&mut draft, &mut verifier, &tokenizer, prompt, 1000, 3)?;

    println!("\nâœ… Total time: {:.2?}", start.elapsed());
    Ok(())
}
