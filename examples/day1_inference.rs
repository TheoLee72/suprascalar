use anyhow::{Error as E, Result};
use candle_core::{Device, Tensor};
use candle_transformers::generation::LogitsProcessor;
// ì¤‘ìš”: ì†ë„ë¥¼ ìœ„í•´ 'quantized_phi3' ëª¨ë“ˆì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
use candle_transformers::models::quantized_phi3::ModelWeights as Phi3;
use hf_hub::api::sync::Api;
use std::io::Write;
use tokenizers::Tokenizer;

#[tokio::main]
async fn main() -> Result<()> {
    println!("ğŸ”¥ Day 1: Suprascalar Inference Engine (Direct Logic)");

    // =========================================================================
    // 1. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ì¤€ë¹„ (HF Hub ì‚¬ìš©)
    // =========================================================================
    let api = Api::new()?;

    // (1) Tokenizer: ì‚¬ìš©ìê°€ ì§€ì •í•œ Microsoft ê³µì‹ Repo ì‚¬ìš©
    let tokenizer_repo_id = "microsoft/Phi-3-mini-4k-instruct";
    println!("ğŸ“¥ Fetching tokenizer from: {}", tokenizer_repo_id);
    let tokenizer_path = api
        .model(tokenizer_repo_id.to_string())
        .get("tokenizer.json")?;
    let tokenizer = Tokenizer::from_file(tokenizer_path).map_err(E::msg)?;

    // (2) Model Weights:
    // Microsoft ê³µì‹ Repoì—ëŠ” 'safetensors(7GB)'ë§Œ ìˆê³  'GGUF(ì–‘ìí™”)' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.
    // ë¡œì»¬ CPUì—ì„œ ë¹ ë¥´ê²Œ ëŒë¦¬ë ¤ë©´ GGUFê°€ í•„ìˆ˜ì´ë¯€ë¡œ,
    // ë™ì¼í•œ ëª¨ë¸ì˜ GGUF ë³€í™˜ ë²„ì „(Bartowski)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    let model_repo_id = "bartowski/Phi-3-mini-4k-instruct-GGUF";
    let model_filename = "Phi-3-mini-4k-instruct-Q4_K_M.gguf";

    println!(
        "ğŸ“¥ Fetching model weights from: {}/{}",
        model_repo_id, model_filename
    );
    let model_path = api.model(model_repo_id.to_string()).get(model_filename)?;

    // =========================================================================
    // 2. ì—”ì§„ ì´ˆê¸°í™” (Boilerplate ì—†ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ëŠ¥ ì§ì ‘ ì‚¬ìš©)
    // =========================================================================
    // let device = Device::Cpu;
    let device = Device::new_cuda(0)?;

    println!("âš™ï¸ Loading GGUF model...");
    let mut file = std::fs::File::open(&model_path)?;
    let model_content = candle_core::quantized::gguf_file::Content::read(&mut file)?;
    let mut model = Phi3::from_gguf(false, model_content, &mut file, &device)?; // disable flash attention (CPU-friendly)

    // Candle ë‚´ì¥ LogitsProcessor (Temperature, Top-P, Seed ì„¤ì •)
    let mut logits_processor = LogitsProcessor::new(299792458, Some(0.7), Some(0.95));

    println!("âœ… Engine Ready!");

    // =========================================================================
    // 3. ì¶”ë¡  ì‹¤í–‰ (Referenceì˜ run() í•¨ìˆ˜ ë¡œì§ì„ mainìœ¼ë¡œ ê°€ì ¸ì˜´)
    // =========================================================================
    let prompt = "<|user|>\nHow to make cake?.<|end|>\n<|assistant|>";
    println!("\nGenerating response for: \n{}", prompt);
    println!("---");

    // (1) Encode
    let tokens = tokenizer.encode(prompt, true).map_err(E::msg)?;
    let mut tokens = tokens.get_ids().to_vec();
    let prompt_len = tokens.len();
    let mut generated_tokens = 0usize;
    let sample_len = 200; // ìµœëŒ€ ìƒì„± ê¸¸ì´

    print!("{}", prompt);
    std::io::stdout().flush()?;
    let mut last_printed = 0usize;

    let start_gen = std::time::Instant::now();

    // [ìˆ˜ì • 1] ìœ„ì¹˜(Position) ì¶”ì  ë³€ìˆ˜ ì„ ì–¸
    let mut pos = 0;

    // [ìˆ˜ì • 2] ì²« ì…ë ¥ì€ 'í”„ë¡¬í”„íŠ¸ ì „ì²´'ì…ë‹ˆë‹¤.
    // Tensor::new(tokens.as_slice()...) -> í”„ë¡¬í”„íŠ¸ ì „ì²´ í† í°
    let mut input = Tensor::new(tokens.as_slice(), &device)?.unsqueeze(0)?;

    // (2) Generation Loop
    for _ in 0..sample_len {
        // [ìˆ˜ì • 3] model.forwardì— 'pos' ì¸ì ì¶”ê°€
        // input: ì´ë²ˆì— ì²˜ë¦¬í•  í† í°ë“¤ (ì²« í„´ì—” í”„ë¡¬í”„íŠ¸ ì „ì²´, ê·¸ ë’¤ë¡  í† í° 1ê°œ)
        // pos: ì´ í† í°ë“¤ì´ ì „ì²´ ë¬¸ì¥ì—ì„œ ì‹œì‘ë˜ëŠ” ìœ„ì¹˜
        let logits = model.forward(&input, pos)?;

        // ë¡œì§“ ì¶”ì¶œ (ë§ˆì§€ë§‰ í† í°ì˜ ì˜ˆì¸¡ê°’)
        let logits = logits.squeeze(0)?; // ëª¨ë¸ì´ ë°°ì¹˜ ì°¨ì›ë§Œ ë‚¨ê¸°ë¯€ë¡œ seq ì°¨ì› ì œê±°

        // Sampling
        let next_token = logits_processor.sample(&logits)?;
        tokens.push(next_token);
        generated_tokens += 1;

        // ëˆ„ì  ë””ì½”ë”©ì„ í†µí•´ ê³µë°±ì„ í¬í•¨í•œ ì›ë¬¸ í˜•íƒœë¥¼ ë³µì›í•œë‹¤.
        let decoded = tokenizer
            .decode(&tokens[prompt_len..], true)
            .map_err(E::msg)?;
        let new_text = &decoded[last_printed..];
        print!("{}", new_text);
        std::io::stdout().flush()?;
        last_printed = decoded.len();

        if next_token == 32000 || next_token == 32007 {
            break;
        }

        // [ìˆ˜ì • 4] ë‹¤ìŒ í„´ ì¤€ë¹„
        // pos ì—…ë°ì´íŠ¸: ë°©ê¸ˆ ì²˜ë¦¬í•œ ì…ë ¥ ê¸¸ì´ë§Œí¼ ë”í•´ì¤Œ (ì²« í„´: í”„ë¡¬í”„íŠ¸ ê¸¸ì´, ì´í›„: 1)
        let (_b, seq_len) = input.dims2()?;
        pos += seq_len;

        // input ì—…ë°ì´íŠ¸: ì´ì œë¶€í„°ëŠ” 'ë°©ê¸ˆ ë§Œë“  í† í° í•˜ë‚˜'ë§Œ ëª¨ë¸ì— ë„£ìŠµë‹ˆë‹¤. (KV Cache í™œìš©)
        input = Tensor::new(&[next_token], &device)?.unsqueeze(0)?;
    }

    let dt = start_gen.elapsed();
    println!(
        "\n\n---\nâš¡ {} tokens generated ({:.2} token/s)",
        generated_tokens,
        generated_tokens as f64 / dt.as_secs_f64(),
    );

    Ok(())
}
